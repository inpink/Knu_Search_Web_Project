# -*- coding: utf-8 -*-
"""Word2Vec CNN-LSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pztiMlNBgSs4FvrpxfbY-EY1dRJTOB_I
"""

from gensim.models import Word2Vec
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# noun_data.txt 파일에서 텍스트를 읽어와서 처리합니다.
noun_data_list = []  # 명사 단어를 저장할 2차원 리스트
class_idx_list = []  # 각 단어의 정해진 분류를 담을 1차원 리스트

with open('noun_data.txt', 'r', encoding='utf-8') as file:
    for line in file:
        # 각 줄을 공백을 기준으로 단어로 분리하여 리스트에 추가합니다.
        words = line.strip().split()
        class_idx_list.append(int(words[0]))
        noun_data_list.append(words[1:])

# 결과 출력
print(noun_data_list[:10])
print(class_idx_list[:10])

# Word2Vec 모델 학습
word2vec_model = Word2Vec(sentences=noun_data_list, vector_size=100, window=5, min_count=2, workers=4, sg=0)

# 데이터를 학습 및 평가 데이터로 분할
X_train, X_test, y_train, y_test = train_test_split(noun_data_list, class_idx_list, test_size=0.2, random_state=42)

# 각 문장을 Word2Vec 벡터로 변환
max_sequence_length = 100  # 시퀀스의 최대 길이

X_train_vectors = []
X_test_vectors = []

for sentence in X_train:
    vectors = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]
    if not vectors:
        # 모든 단어가 Word2Vec 모델에 없는 경우 처리
        vectors = [np.zeros(word2vec_model.vector_size)]
    X_train_vectors.append(vectors)

for sentence in X_test:
    vectors = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]
    if not vectors:
        # 모든 단어가 Word2Vec 모델에 없는 경우 처리
        vectors = [np.zeros(word2vec_model.vector_size)]
    X_test_vectors.append(vectors)

# 패딩
X_train_pad = pad_sequences(X_train_vectors, maxlen=max_sequence_length, dtype='float32')
X_test_pad = pad_sequences(X_test_vectors, maxlen=max_sequence_length, dtype='float32')

# 레이블 데이터를 NumPy 배열로 변환
y_train = np.array(y_train)
y_test = np.array(y_test)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3,
                                 padding='same', activation='relu',
                                 input_shape=(max_sequence_length, word2vec_model.vector_size)))
model.add(tf.keras.layers.MaxPooling1D(pool_size=2))
model.add(tf.keras.layers.LSTM(100))
model.add(tf.keras.layers.Dense(5, activation='softmax'))  # 5개의 클래스로 변경

# 모델 컴파일
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# 모델 학습
model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=2, batch_size=64)

# 모델 평가
loss, accuracy = model.evaluate(X_test_pad, y_test)
print(f'평가 손실: {loss}, 평가 정확도: {accuracy}')

def input_model_test(input_text, word2vec_model, model):
    input_text = input_text.split()
    input_vectors = []
    for word in input_text:
        if word in word2vec_model.wv:
            vector = word2vec_model.wv[word]
            input_vectors.append(vector)
        else:
            input_vectors.append(np.zeros(word2vec_model.vector_size))

    input_vectors = pad_sequences([input_vectors], maxlen=max_sequence_length, dtype='float32')

    # 모델 예측
    predictions = model.predict(input_vectors)
    class_prediction = np.argmax(predictions, axis=-1)
    print(input_text)
    print(f'입력 텍스트의 예측 클래스: {class_prediction[0]}')
    print(f'예측 확률: {predictions[0]}')
    print()

    return class_prediction[0]

input_model_test("사랑 나눔 봉사",word2vec_model,model)

input_model_test("중간 고사 시험",word2vec_model,model)

input_model_test("직원 모집",word2vec_model,model)

input_model_test("연구원 채용",word2vec_model,model)

input_model_test("학습 공동체",word2vec_model,model)

input_model_test("소프트웨어전공 취업",word2vec_model,model)

input_model_test("코멘토",word2vec_model,model)

input_model_test("국가장학금 신청",word2vec_model,model)

input_model_test("방학 CTL",word2vec_model,model)

model.save('my_model.h5')  # HDF5 파일로 모델 저장
word2vec_model.save('word2vec_model.model')  # Word2Vec 모델 저장


